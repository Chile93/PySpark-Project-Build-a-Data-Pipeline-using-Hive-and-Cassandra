{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e88af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# nstall pyspark library\n",
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cae4df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (24.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.0/1.8 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-25.3\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a292b0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: py4j<0.10.9.10,>=0.10.9.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b13a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "438f4530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyspark 3.4.1\n",
      "Uninstalling pyspark-3.4.1:\n",
      "  Successfully uninstalled pyspark-3.4.1\n",
      "Found existing installation: pyspark 3.4.1\n",
      "Uninstalling pyspark-3.4.1:\n",
      "  Successfully uninstalled pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pyspark -y\n",
    "# !pip install pyspark==3.4.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca47bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.7\n",
      "  Using cached pyspark-3.5.7.tar.gz (317.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyspark==3.5.7) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): still running...\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.7-py2.py3-none-any.whl size=317907817 sha256=5b5b3637902fffe7423bebfe26a4a2bc31fd59e9cf463d01f28d9412e3102f69\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\58\\93\\ef\\df0fa5d76db2fedf77481d96ab4ae9b9a77eb52fbf2a65b727\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.7\n",
      "Collecting pyspark==3.5.7\n",
      "  Using cached pyspark-3.5.7.tar.gz (317.4 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyspark==3.5.7) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): still running...\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.7-py2.py3-none-any.whl size=317907817 sha256=5b5b3637902fffe7423bebfe26a4a2bc31fd59e9cf463d01f28d9412e3102f69\n",
      "  Stored in directory: c:\\users\\lenovo\\appdata\\local\\pip\\cache\\wheels\\58\\93\\ef\\df0fa5d76db2fedf77481d96ab4ae9b9a77eb52fbf2a65b727\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a790589c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.7\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, Java HotSpot(TM) 64-Bit Server VM, 11.0.28\n",
      "Branch HEAD\n",
      "Compiled by user runner on 2025-09-17T20:37:30Z\n",
      "Revision ed00d046951a7ecda6429accd3b9c5b2dc792b65\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fe4d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.7\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de5d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354dec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PYSPARK_PIN_THREAD\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f710fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spark session library\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aff9d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691f3cd",
   "metadata": {},
   "source": [
    "# Create spark session and set hive metastore property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3ae9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://54.211.164.218:9083\")\n",
    "sparkSession = (SparkSession\n",
    "                .builder\n",
    "                .appName('integration-pyspark-hive1')\n",
    "                .enableHiveSupport()\n",
    "                .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2915953",
   "metadata": {},
   "source": [
    "# check what databases in available in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b02863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|airline_flight|\n",
      "|       default|\n",
      "|       flight1|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac62866b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparkSession.sql('create database airline_flight').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd849c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark datafrme from csv file\n",
    "AirlineDF = sparkSession.read.option(\"header\", \"true\").csv(\"airlines1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab738536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+-----+----------+---------+----------+-----------------+------------------------+---------------------------+-----------+-------------------------------+---------------+------------------+------------------+------+--------------------+-----------+---------------+---------------+---------+-------------+----------------+----------------+----+--------------------+---------+-------------+--------------+-------+----------+-------+--------+---------------+--------+--------------------+----------+-------+---------+--------+------+----------+-------+--------+---------------+--------+------------------+----------+---------+----------------+--------+--------------+-----------------+-------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+------------+-------------+---------------+------------------+--------------+--------------------+-----------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+\n",
      "|_c0|Year|Quarter|Month|DayofMonth|DayOfWeek|FlightDate|Reporting_Airline|DOT_ID_Reporting_Airline|IATA_CODE_Reporting_Airline|Tail_Number|Flight_Number_Reporting_Airline|OriginAirportID|OriginAirportSeqID|OriginCityMarketID|Origin|      OriginCityName|OriginState|OriginStateFips|OriginStateName|OriginWac|DestAirportID|DestAirportSeqID|DestCityMarketID|Dest|        DestCityName|DestState|DestStateFips| DestStateName|DestWac|CRSDepTime|DepTime|DepDelay|DepDelayMinutes|DepDel15|DepartureDelayGroups|DepTimeBlk|TaxiOut|WheelsOff|WheelsOn|TaxiIn|CRSArrTime|ArrTime|ArrDelay|ArrDelayMinutes|ArrDel15|ArrivalDelayGroups|ArrTimeBlk|Cancelled|CancellationCode|Diverted|CRSElapsedTime|ActualElapsedTime|AirTime|Flights|Distance|DistanceGroup|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|FirstDepTime|TotalAddGTime|LongestAddGTime|DivAirportLandings|DivReachedDest|DivActualElapsedTime|DivArrDelay|DivDistance|Div1Airport|Div1AirportID|Div1AirportSeqID|Div1WheelsOn|Div1TotalGTime|Div1LongestGTime|Div1WheelsOff|Div1TailNum|Div2Airport|Div2AirportID|Div2AirportSeqID|Div2WheelsOn|Div2TotalGTime|Div2LongestGTime|Div2WheelsOff|Div2TailNum|Div3Airport|Div3AirportID|Div3AirportSeqID|Div3WheelsOn|Div3TotalGTime|Div3LongestGTime|Div3WheelsOff|Div3TailNum|Div4Airport|Div4AirportID|Div4AirportSeqID|Div4WheelsOn|Div4TotalGTime|Div4LongestGTime|Div4WheelsOff|Div4TailNum|Div5Airport|Div5AirportID|Div5AirportSeqID|Div5WheelsOn|Div5TotalGTime|Div5LongestGTime|Div5WheelsOff|Div5TailNum|\n",
      "+---+----+-------+-----+----------+---------+----------+-----------------+------------------------+---------------------------+-----------+-------------------------------+---------------+------------------+------------------+------+--------------------+-----------+---------------+---------------+---------+-------------+----------------+----------------+----+--------------------+---------+-------------+--------------+-------+----------+-------+--------+---------------+--------+--------------------+----------+-------+---------+--------+------+----------+-------+--------+---------------+--------+------------------+----------+---------+----------------+--------+--------------+-----------------+-------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+------------+-------------+---------------+------------------+--------------+--------------------+-----------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+\n",
      "|  0|1998|      1|    1|         2|        5|1998-01-02|               NW|                   19386|                         NW|     N297US|                            675|          13487|           1348701|             31650|   MSP|     Minneapolis, MN|         MN|           27.0|      Minnesota|       63|        14869|         1486902|           34614| SLC|  Salt Lake City, UT|       UT|         49.0|          Utah|     87|      1640| 1659.0|    19.0|           19.0|     1.0|                 1.0| 1600-1659|   24.0|   1723.0|  1856.0|   3.0|      1836| 1859.0|    23.0|           23.0|     1.0|               1.0| 1800-1859|      0.0|            NULL|     0.0|         176.0|            180.0|  153.0|    1.0|   991.0|            4|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  1|2009|      2|    5|        28|        4|2009-05-28|               FL|                   20437|                         FL|     N946AT|                            671|          13342|           1334202|             33342|   MKE|       Milwaukee, WI|         WI|           55.0|      Wisconsin|       45|        13204|         1320401|           31454| MCO|         Orlando, FL|       FL|         12.0|       Florida|     33|      1204| 1202.0|    -2.0|            0.0|     0.0|                -1.0| 1200-1259|   10.0|   1212.0|  1533.0|   8.0|      1541| 1541.0|     0.0|            0.0|     0.0|               0.0| 1500-1559|      0.0|            NULL|     0.0|         157.0|            159.0|  141.0|    1.0|  1066.0|            5|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  2|2013|      2|    6|        29|        6|2013-06-29|               MQ|                   20398|                         MQ|     N665MQ|                           3297|          11921|           1192102|             31921|   GJT|  Grand Junction, CO|         CO|            8.0|       Colorado|       82|        11298|         1129803|           30194| DFW|Dallas/Fort Worth...|       TX|         48.0|         Texas|     74|      1630| 1644.0|    14.0|           14.0|     0.0|                 0.0| 1600-1659|    9.0|   1653.0|  1936.0|   6.0|      1945| 1942.0|    -3.0|            0.0|     0.0|              -1.0| 1900-1959|      0.0|            NULL|     0.0|         135.0|            118.0|  103.0|    1.0|   773.0|            4|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  3|2010|      3|    8|        31|        2|2010-08-31|               DL|                   19790|                         DL|     N6705Y|                           1806|          12892|           1289201|             32575|   LAX|     Los Angeles, CA|         CA|            6.0|     California|       91|        11433|         1143301|           31295| DTW|         Detroit, MI|       MI|         26.0|      Michigan|     43|      1305| 1305.0|     0.0|            0.0|     0.0|                 0.0| 1300-1359|   23.0|   1328.0|  2008.0|   7.0|      2035| 2015.0|   -20.0|            0.0|     0.0|              -2.0| 2000-2059|      0.0|            NULL|     0.0|         270.0|            250.0|  220.0|    1.0|  1979.0|            8|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  4|2006|      1|    1|        15|        7|2006-01-15|               US|                   20355|                         US|     N504AU|                            465|          11618|           1161801|             31703|   EWR|          Newark, NJ|         NJ|           34.0|     New Jersey|       21|        11057|         1105702|           31057| CLT|       Charlotte, NC|       NC|         37.0|North Carolina|     36|      1820| 1911.0|    51.0|           51.0|     1.0|                 3.0| 1800-1859|   19.0|   1930.0|  2050.0|   8.0|      2026| 2058.0|    32.0|           32.0|     1.0|               2.0| 2000-2059|      0.0|            NULL|     0.0|         126.0|            107.0|   80.0|    1.0|   529.0|            3|         0.0|         0.0|     0.0|          0.0|             32.0|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  5|1995|      4|   11|        29|        3|1995-11-29|               DL|                   19790|                         DL|     N925DL|                           1198|          11298|           1129802|             30194|   DFW|Dallas/Fort Worth...|         TX|           48.0|          Texas|       74|        14814|         1481401|           30476| SHV|      Shreveport, LA|       LA|         22.0|     Louisiana|     72|       639|  639.0|     0.0|            0.0|     0.0|                 0.0| 0600-0659|   29.0|    708.0|   736.0|   5.0|       730|  741.0|    11.0|           11.0|     0.0|               0.0| 0700-0759|      0.0|            NULL|     0.0|          51.0|             62.0|   28.0|    1.0|   190.0|            1|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  6|2006|      3|    8|         7|        1|2006-08-07|               CO|                   19704|                         CO|     N27724|                           1431|          10721|           1072101|             30721|   BOS|          Boston, MA|         MA|           25.0|  Massachusetts|       13|        11042|         1104201|           30647| CLE|       Cleveland, OH|       OH|         39.0|          Ohio|     44|      1755| 1751.0|    -4.0|            0.0|     0.0|                -1.0| 1700-1759|   33.0|   1824.0|  1958.0|   4.0|      2000| 2002.0|     2.0|            2.0|     0.0|               0.0| 2000-2059|      0.0|            NULL|     0.0|         125.0|            131.0|   94.0|    1.0|   563.0|            3|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  7|2019|      2|    6|        11|        2|2019-06-11|               9E|                   20363|                         9E|     N927XJ|                           3459|          10397|           1039707|             30397|   ATL|         Atlanta, GA|         GA|           13.0|        Georgia|       34|        10868|         1086803|           30868| CAE|        Columbia, SC|       SC|         45.0|South Carolina|     37|      1950| 2331.0|   221.0|          221.0|     1.0|                12.0| 1900-1959|   19.0|   2350.0|    25.0|   6.0|      2057|   31.0|   214.0|          214.0|     1.0|              12.0| 2000-2059|      0.0|            NULL|     0.0|          67.0|             60.0|   35.0|    1.0|   192.0|            1|         0.0|         0.0|     0.0|          0.0|            214.0|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  8|2008|      3|    8|         3|        7|2008-08-03|               YV|                   20378|                         YV|     N522LR|                           7233|          13930|           1393001|             30977|   ORD|         Chicago, IL|         IL|           17.0|       Illinois|       41|        11042|         1104201|           30647| CLE|       Cleveland, OH|       OH|         39.0|          Ohio|     44|      1550| 1552.0|     2.0|            2.0|     0.0|                 0.0| 1500-1559|   26.0|   1618.0|  1817.0|   3.0|      1810| 1820.0|    10.0|           10.0|     0.0|               0.0| 1800-1859|      0.0|            NULL|     0.0|          80.0|             88.0|   59.0|    1.0|   316.0|            2|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "|  9|2018|      1|    2|         8|        4|2018-02-08|               WN|                   19393|                         WN|     N8688J|                           5932|          13232|           1323202|             30977|   MDW|         Chicago, IL|         IL|           17.0|       Illinois|       41|        11259|         1125903|           30194| DAL|          Dallas, TX|       TX|         48.0|         Texas|     74|      2030| 2046.0|    16.0|           16.0|     1.0|                 1.0| 2000-2059|   34.0|   2120.0|  2314.0|   5.0|      2250| 2319.0|    29.0|           29.0|     1.0|               1.0| 2200-2259|      0.0|            NULL|     0.0|         140.0|            153.0|  114.0|    1.0|   793.0|            4|         0.0|         0.0|    13.0|          0.0|             16.0|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 10|1991|      4|   11|        21|        4|1991-11-21|               US|                   20355|                         US|       NULL|                           2135|          14679|           1467902|             33570|   SAN|       San Diego, CA|         CA|            6.0|     California|       91|        12892|         1289201|           32575| LAX|     Los Angeles, CA|       CA|          6.0|    California|     91|      1245| 1247.0|     2.0|            2.0|     0.0|                 0.0| 1200-1259|   NULL|     NULL|    NULL|  NULL|      1325| 1331.0|     6.0|            6.0|     0.0|               0.0| 1300-1359|      0.0|            NULL|     0.0|          40.0|             44.0|   NULL|    1.0|   109.0|            1|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 11|2014|      2|    4|         3|        4|2014-04-03|               WN|                   19393|                         WN|     N374SW|                           1463|          11540|           1154003|             30615|   ELP|         El Paso, TX|         TX|           48.0|          Texas|       74|        11259|         1125903|           30194| DAL|          Dallas, TX|       TX|         48.0|         Texas|     74|      1025| 1025.0|     0.0|            0.0|     0.0|                 0.0| 1000-1059|   10.0|   1035.0|  1252.0|   3.0|      1300| 1255.0|    -5.0|            0.0|     0.0|              -1.0| 1300-1359|      0.0|            NULL|     0.0|          95.0|             90.0|   77.0|    1.0|   562.0|            3|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 12|1994|      3|    7|        24|        7|1994-07-24|               AA|                   19805|                         AA|       NULL|                            830|          14843|           1484302|             34819|   SJU|        San Juan, PR|         PR|           72.0|    Puerto Rico|        3|        13303|         1330302|           32467| MIA|           Miami, FL|       FL|         12.0|       Florida|     33|      1245| 1242.0|    -3.0|            0.0|     0.0|                -1.0| 1200-1259|   NULL|     NULL|    NULL|  NULL|      1521| 1511.0|   -10.0|            0.0|     0.0|              -1.0| 1500-1559|      0.0|            NULL|     0.0|         156.0|            149.0|   NULL|    1.0|  1045.0|            5|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 13|2013|      2|    6|        30|        7|2013-06-30|               OO|                   20304|                         OO|     N947SW|                           6474|          10140|           1014002|             30140|   ABQ|     Albuquerque, NM|         NM|           35.0|     New Mexico|       86|        12892|         1289203|           32575| LAX|     Los Angeles, CA|       CA|          6.0|    California|     91|      1601| 1557.0|    -4.0|            0.0|     0.0|                -1.0| 1600-1659|    6.0|   1603.0|  1638.0|   8.0|      1705| 1646.0|   -19.0|            0.0|     0.0|              -2.0| 1700-1759|      0.0|            NULL|     0.0|         124.0|            109.0|   95.0|    1.0|   677.0|            3|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 14|2003|      4|   11|        21|        5|2003-11-21|               UA|                   19977|                         UA|     N308UA|                            674|          13930|           1393001|             30977|   ORD|         Chicago, IL|         IL|           17.0|       Illinois|       41|        12953|         1295301|           31703| LGA|        New York, NY|       NY|         36.0|      New York|     22|       900|  855.0|    -5.0|            0.0|     0.0|                -1.0| 0900-0959|   14.0|    909.0|  1148.0|   2.0|      1206| 1150.0|   -16.0|            0.0|     0.0|              -2.0| 1200-1259|      0.0|            NULL|     0.0|         126.0|            115.0|   99.0|    1.0|   733.0|            3|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 15|1988|      2|    4|         4|        1|1988-04-04|               PI|                   19822|                         PI|       NULL|                            362|          11995|           1199501|             31995|   GSO|Greensboro/High P...|         NC|           37.0| North Carolina|       36|        10821|         1082102|           30852| BWI|       Baltimore, MD|       MD|         24.0|      Maryland|     35|       630|  631.0|     1.0|            1.0|     0.0|                 0.0| 0600-0659|   NULL|     NULL|    NULL|  NULL|       728|  737.0|     9.0|            9.0|     0.0|               0.0| 0700-0759|      0.0|            NULL|     0.0|          58.0|             66.0|   NULL|    1.0|   278.0|            2|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 16|2007|      3|    9|         5|        3|2007-09-05|               NW|                   19386|                         NW|     N8928E|                           1678|          11433|           1143301|             31295|   DTW|         Detroit, MI|         MI|           26.0|       Michigan|       43|        13184|         1318402|           33184| MBS|Saginaw/Bay City/...|       MI|         26.0|      Michigan|     43|      1201| 1150.0|   -11.0|            0.0|     0.0|                -1.0| 1200-1259|   20.0|   1210.0|  1234.0|   5.0|      1246| 1239.0|    -7.0|            0.0|     0.0|              -1.0| 1200-1259|      0.0|            NULL|     0.0|          45.0|             49.0|   24.0|    1.0|    98.0|            1|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 17|2015|      3|    9|         5|        6|2015-09-05|               AS|                   19930|                         AS|     N589AS|                            413|          14869|           1486903|             34614|   SLC|  Salt Lake City, UT|         UT|           49.0|           Utah|       87|        14747|         1474703|           30559| SEA|         Seattle, WA|       WA|         53.0|    Washington|     93|      1745| 1733.0|   -12.0|            0.0|     0.0|                -1.0| 1700-1759|    9.0|   1742.0|  1824.0|   8.0|      1855| 1832.0|   -23.0|            0.0|     0.0|              -2.0| 1800-1859|      0.0|            NULL|     0.0|         130.0|            119.0|  102.0|    1.0|   689.0|            3|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 18|2006|      1|    1|        28|        6|2006-01-28|               UA|                   19977|                         UA|     N426UA|                            210|          12892|           1289201|             32575|   LAX|     Los Angeles, CA|         CA|            6.0|     California|       91|        12264|         1226401|           30852| IAD|      Washington, DC|       VA|         51.0|      Virginia|     38|      1545| 1542.0|    -3.0|            0.0|     0.0|                -1.0| 1500-1559|   11.0|   1553.0|  2308.0|   5.0|      2327| 2313.0|   -14.0|            0.0|     0.0|              -1.0| 2300-2359|      0.0|            NULL|     0.0|         282.0|            271.0|  255.0|    1.0|  2288.0|           10|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|              NULL|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "| 19|2017|      3|    7|        26|        3|2017-07-26|               WN|                   19393|                         WN|     N252WN|                            995|          14893|           1489302|             33192|   SMF|      Sacramento, CA|         CA|            6.0|     California|       91|        12892|         1289205|           32575| LAX|     Los Angeles, CA|       CA|          6.0|    California|     91|      1535|   NULL|    NULL|           NULL|    NULL|                NULL| 1500-1559|   NULL|     NULL|    NULL|  NULL|      1700|   NULL|    NULL|           NULL|    NULL|              NULL| 1700-1759|      1.0|               A|     0.0|          85.0|             NULL|   NULL|    1.0|   373.0|            2|        NULL|        NULL|    NULL|         NULL|             NULL|        NULL|         NULL|           NULL|               0.0|          NULL|                NULL|       NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|       NULL|         NULL|            NULL|        NULL|          NULL|            NULL|         NULL|       NULL|\n",
      "+---+----+-------+-----+----------+---------+----------+-----------------+------------------------+---------------------------+-----------+-------------------------------+---------------+------------------+------------------+------+--------------------+-----------+---------------+---------------+---------+-------------+----------------+----------------+----+--------------------+---------+-------------+--------------+-------+----------+-------+--------+---------------+--------+--------------------+----------+-------+---------+--------+------+----------+-------+--------+---------------+--------+------------------+----------+---------+----------------+--------+--------------+-----------------+-------+-------+--------+-------------+------------+------------+--------+-------------+-----------------+------------+-------------+---------------+------------------+--------------+--------------------+-----------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+-----------+-------------+----------------+------------+--------------+----------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AirlineDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27221a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe and select some columns\n",
    "AirlineDF1 = AirlineDF.select(\"Year\",\"Reporting_Airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ddf8362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|Year|Reporting_Airline|\n",
      "+----+-----------------+\n",
      "|1998|               NW|\n",
      "|2009|               FL|\n",
      "|2013|               MQ|\n",
      "|2010|               DL|\n",
      "|2006|               US|\n",
      "|1995|               DL|\n",
      "|2006|               CO|\n",
      "|2019|               9E|\n",
      "|2008|               YV|\n",
      "|2018|               WN|\n",
      "|1991|               US|\n",
      "|2014|               WN|\n",
      "|1994|               AA|\n",
      "|2013|               OO|\n",
      "|2003|               UA|\n",
      "|1988|               PI|\n",
      "|2007|               NW|\n",
      "|2015|               AS|\n",
      "|2006|               UA|\n",
      "|2017|               WN|\n",
      "+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AirlineDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97d202f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME: C:\\hadoop\n",
      "SPARK_HOME: C:\\spark-3.5.7-bin-hadoop3\n",
      "PATH contains hadoop: True\n",
      "winutils exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"HADOOP_HOME:\", os.environ.get('HADOOP_HOME'))\n",
    "print(\"SPARK_HOME:\", os.environ.get('SPARK_HOME'))\n",
    "print(\"PATH contains hadoop:\", \"hadoop\" in os.environ.get('PATH', ''))\n",
    "\n",
    "# Check if winutils exists\n",
    "if os.environ.get('HADOOP_HOME'):\n",
    "    winutils_path = os.path.join(os.environ['HADOOP_HOME'], 'bin', 'winutils.exe')\n",
    "    print(\"winutils exists:\", os.path.exists(winutils_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cfd43ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated PATH with Hadoop bin directory\n",
      "HADOOP_HOME: C:\\hadoop\n",
      "PATH now contains hadoop: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set Hadoop environment variables\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "\n",
    "# Add Hadoop to PATH\n",
    "hadoop_bin = os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "current_path = os.environ.get('PATH', '')\n",
    "if hadoop_bin not in current_path:\n",
    "    os.environ['PATH'] = hadoop_bin + os.pathsep + current_path\n",
    "    print(\"Updated PATH with Hadoop bin directory\")\n",
    "    print(\"HADOOP_HOME:\", os.environ.get('HADOOP_HOME'))\n",
    "    print(\"PATH now contains hadoop:\", \"hadoop\" in os.environ.get('PATH', '').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd32026",
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkRuntimeException",
     "evalue": "[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`airline_flight`.`airline`, as its associated location 'file:/C:/Users/LENOVO/Desktop/BigData%20Project/AWS/PySpark%20Project-Build%20a%20Data%20Pipeline%20using%20Hive%20and%20Cassandra/Code/Code/spark-warehouse/airline_flight.db/airline' already exists. Please pick a different table name, or remove the existing location first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mAirlineDF1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mairline_flight.airline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# AirlineDF1.write.mode(\"overwrite\").saveAsTable(\"airline_flight.airline\")\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mSparkRuntimeException\u001b[0m: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`airline_flight`.`airline`, as its associated location 'file:/C:/Users/LENOVO/Desktop/BigData%20Project/AWS/PySpark%20Project-Build%20a%20Data%20Pipeline%20using%20Hive%20and%20Cassandra/Code/Code/spark-warehouse/airline_flight.db/airline' already exists. Please pick a different table name, or remove the existing location first."
     ]
    }
   ],
   "source": [
    "AirlineDF1.write.saveAsTable(\"airline_flight.airline\")\n",
    "# AirlineDF1.write.mode(\"overwrite\").saveAsTable(\"airline_flight.airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9473ee88",
   "metadata": {},
   "outputs": [
    {
     "ename": "SparkRuntimeException",
     "evalue": "[LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`airline_flight`.`airline`, as its associated location 'file:/C:/Users/LENOVO/Desktop/BigData%20Project/AWS/PySpark%20Project-Build%20a%20Data%20Pipeline%20using%20Hive%20and%20Cassandra/Code/Code/spark-warehouse/airline_flight.db/airline' already exists. Please pick a different table name, or remove the existing location first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m sparkSession\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS airline_flight.airline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mAirlineDF1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mairline_flight.airline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mSparkRuntimeException\u001b[0m: [LOCATION_ALREADY_EXISTS] Cannot name the managed table as `spark_catalog`.`airline_flight`.`airline`, as its associated location 'file:/C:/Users/LENOVO/Desktop/BigData%20Project/AWS/PySpark%20Project-Build%20a%20Data%20Pipeline%20using%20Hive%20and%20Cassandra/Code/Code/spark-warehouse/airline_flight.db/airline' already exists. Please pick a different table name, or remove the existing location first."
     ]
    }
   ],
   "source": [
    "\n",
    "sparkSession.sql(\"DROP TABLE IF EXISTS airline_flight.airline\")\n",
    "AirlineDF1.write.saveAsTable(\"airline_flight.airline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45d65a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|airline_flight|\n",
      "|       default|\n",
      "|       flight1|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c0ffc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql(\"use airline_flight\").show()\n",
    "sparkSession.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64706df4",
   "metadata": {},
   "source": [
    "# create database in hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c5a6cc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[SCHEMA_ALREADY_EXISTS] Cannot create schema `flight1` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreate database flight1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[1;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[0;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[0;32m   1630\u001b[0m         )\n\u001b[1;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [SCHEMA_ALREADY_EXISTS] Cannot create schema `flight1` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema."
     ]
    }
   ],
   "source": [
    "sparkSession.sql(\"create database flight1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8a0e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|airline_flight|\n",
      "|       default|\n",
      "|       flight1|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a2c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkSession.sql(\"use flight1\").show()\n",
    "sparkSession.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53622416",
   "metadata": {},
   "source": [
    "# Read write data from spark to hive table\n",
    "# we need database which is already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "036c838c",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o67.saveAsTable.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:511)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:229)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:183)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:710)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:688)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:582)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mAirlineDF1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflight1.air3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[1;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m-> 1521\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o67.saveAsTable.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:511)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:229)\r\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:183)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:710)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:688)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:582)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "AirlineDF1.write.saveAsTable(\"flight1.air3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc39954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql(\"use flight1\").show()\n",
    "sparkSession.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e64f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql(\"select * from air3\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870cb58",
   "metadata": {},
   "source": [
    "# create table from pyspark to hive and apply some sql oprations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql(\"create database test11\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50720b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('use test11').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243b0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('create table people (id int,name string)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('insert into people (id,name) values (100,\"tom\")').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('insert into people (id,name) values (102,\"jack\")').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11dd51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('select * from people').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sparkSession.sql('select * from people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d84f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.mode(\"overwrite\").saveAsTable(\"test11.people1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5afdc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('select * from people1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0526f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('drop table people1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2097d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop database error comes because some tables in available in database\n",
    "sparkSession.sql('drop database test11').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec026398",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('drop table people').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('drop database test11').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e46316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
